{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "seed = 1729\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_dataset(data, variable_indexes, lookback_period, step, forecast_period, motif_indexes):\n",
    "    X1, X2, y = [], [], []  # X1: data, X2: indexes of the motifs, y: distance to the next motif\n",
    "    \n",
    "    for idx in range(len(data[0]) - lookback_period - 1):\n",
    "        if idx % step != 0:\n",
    "            continue\n",
    "\n",
    "        window_end_idx = idx + lookback_period\n",
    "        forecast_period_end = window_end_idx + forecast_period\n",
    "\n",
    "        # If there are no more matches after the window, break\n",
    "        if not any([window_end_idx < motif_idx for motif_idx in motif_indexes]):\n",
    "            break\n",
    "\n",
    "        # Motif indexes in window, relative to the start of the window\n",
    "        motif_indexes_in_window = [motif_idx - idx for motif_idx in motif_indexes if idx <= motif_idx <= window_end_idx]\n",
    "        motif_indexes_in_forecast_period = [motif_idx for motif_idx in motif_indexes if window_end_idx < motif_idx <= forecast_period_end]\n",
    "\n",
    "        if motif_indexes_in_forecast_period:\n",
    "            next_match_in_forecast_period = motif_indexes_in_forecast_period[0]\n",
    "        else:\n",
    "            next_match_in_forecast_period = -1  # No match in the forecast period but exists in the future\n",
    "\n",
    "        # Get the data window and transpose to (lookback_period, num_features)\n",
    "        data_window = data[variable_indexes, idx:window_end_idx].T\n",
    "\n",
    "        # Calculate `y`\n",
    "        data_y = -1\n",
    "        if next_match_in_forecast_period != -1:\n",
    "            # Index of the next match relative to the end of the window\n",
    "            data_y = next_match_in_forecast_period - window_end_idx\n",
    "        \n",
    "        # Append to lists\n",
    "        X1.append(torch.tensor(data_window, dtype=torch.float32))  # Now with shape (lookback_period, num_features)\n",
    "        X2.append(torch.tensor(motif_indexes_in_window, dtype=torch.long)) \n",
    "        y.append(data_y) \n",
    "\n",
    "    # Pad X2 sequences to have the same length\n",
    "    X2_padded = pad_sequence(X2, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    # Convert lists to torch tensors\n",
    "    X1 = torch.stack(X1)  # Final shape: (num_samples, lookback_period, num_features)\n",
    "    y = torch.tensor(y, dtype=torch.float32).unsqueeze(1) \n",
    "\n",
    "    return X1, X2_padded, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "n = 100000 #number of data points\n",
    "k = 3 #number of variables\n",
    "p = 5 # pattern length\n",
    "variable_indexes = range(k)\n",
    "\n",
    "data_scenario3 = np.genfromtxt(\"../data/syntheticdata/scenario3_n={}_k={}_p={}_min_step={}_max_step={}.csv\".format(n, k, p, 5, 45), delimiter=\",\").astype(int).reshape((k, n))\n",
    "\n",
    "motif_indexes_scenario3 = np.genfromtxt(\"../data/syntheticdata/motif_indexes_scenario3_n={}_k={}_p={}_min_step={}_max_step={}.csv\".format(n, k, p, 5, 45), delimiter=\",\").astype(int)\n",
    "\n",
    "print(motif_indexes_scenario3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseries_split import BlockingTimeSeriesSplit\n",
    "\n",
    "#create index  \n",
    "indexes = np.arange(len(data_scenario3[0]))\n",
    "\n",
    "#split data\n",
    "tscv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(indexes)):\n",
    "    # Plot train and test indices\n",
    "    ax.plot(train_index, np.zeros_like(train_index) + i, 'o', color='lightblue')\n",
    "    ax.plot(test_index, np.zeros_like(test_index) + i, 'o', color='red')\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    \n",
    "\n",
    "ax.set_yticks(np.arange(5), [\"Fold {}\".format(i) for i in range(1, 6)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_period = 100 #window size\n",
    "step = 5 #step size for the sliding window\n",
    "forecast_period = 50 #forward window size\n",
    "\n",
    "#x1: past window, x2: indexes of the motif in the window,  y: next relative index of the motif\n",
    "X1, X2, y = create_dataset(data_scenario3, variable_indexes, lookback_period, step, forecast_period, motif_indexes_scenario3)\n",
    "\n",
    "#X1 shape is (num_samples, lookback_period, num_features)\n",
    "masking_X1 = np.zeros((X1.shape[0], X1.shape[1], 1)) \n",
    "\n",
    "for i, obs_motif_indexes in enumerate(X2):\n",
    "    for j, idx in enumerate(obs_motif_indexes):\n",
    "        masking_X1[i, idx.item():idx.item()+p, 0] = 1\n",
    "\n",
    "masking_X1 = torch.tensor(masking_X1, dtype=torch.float32)\n",
    "\n",
    "# X1, X2, and y are now PyTorch tensors\n",
    "print(\"X1 shape:\", X1.shape)  # Expected shape: (num_samples, lookback_period, num_features)\n",
    "print(\"masking_X1 shape:\", masking_X1.shape)  # Expected shape: (num_samples, lookback_period, 1)\n",
    "print(\"X2 shape:\", X2.shape)  # Expected shape: (num_samples, max_motif_length_in_window)\n",
    "print(\"y shape:\", y.shape)    # Expected shape: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import optuna\n",
    "import joblib\n",
    "from typing import Tuple, List\n",
    "import csv\n",
    "from models.lstm_pytorch import LSTMX1Input, LSTMX1_X2BeforeLSTM, LSTMX1_X2AfterLSTM\n",
    "\n",
    "# Helper classes and functions\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def evaluate_metrics(predictions: torch.Tensor, targets: torch.Tensor) -> Tuple[float, float]:\n",
    "    mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "    rmse = torch.sqrt(torch.mean((predictions - targets) ** 2)).item()\n",
    "    return mae, rmse\n",
    "\n",
    "# Scale training data and apply the scaler to validation and test data\n",
    "def scale_data(X_train, X_val, X_test):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_train = torch.tensor(scaler.fit_transform(X_train.view(-1, X_train.shape[-1])), dtype=torch.float32).view(X_train.shape)\n",
    "    X_val = torch.tensor(scaler.transform(X_val.view(-1, X_val.shape[-1])), dtype=torch.float32).view(X_val.shape)\n",
    "    X_test = torch.tensor(scaler.transform(X_test.view(-1, X_test.shape[-1])), dtype=torch.float32).view(X_test.shape)\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "\n",
    "def train_model_single_input(model, criterion, optimizer, train_loader, val_loader, num_epochs=1000):\n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=1e-5)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None  # Store the state of the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X1, batch_y in train_loader:\n",
    "            batch_X1, batch_y = batch_X1.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_X1), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X1, batch_y in val_loader:\n",
    "                batch_X1, batch_y = batch_X1.to(device), batch_y.to(device)\n",
    "                val_loss += criterion(model(batch_X1), batch_y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Check early stopping criteria\n",
    "        if epoch >= 100 and early_stopper.early_stop(avg_val_loss):\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()  # Save best model state\n",
    "\n",
    "    # Load the best model state before returning, to ensure best validation performance\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_val_loss, model\n",
    "\n",
    "\n",
    "def train_model_dual_input(model, criterion, optimizer, train_loader, val_loader, num_epochs=1000):\n",
    "    early_stopper = EarlyStopper(patience=10, min_delta=1e-5)\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None  # Store the state of the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X1, batch_X2, batch_y in train_loader:\n",
    "            batch_X1, batch_X2, batch_y = batch_X1.to(device), batch_X2.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(batch_X1, batch_X2), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X1, batch_X2, batch_y in val_loader:\n",
    "                batch_X1, batch_X2, batch_y = batch_X1.to(device), batch_X2.to(device), batch_y.to(device)\n",
    "                val_loss += criterion(model(batch_X1, batch_X2), batch_y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Check early stopping criteria\n",
    "        if epoch >= 100 and early_stopper.early_stop(avg_val_loss):\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Save the best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()  # Save best model state\n",
    "\n",
    "    # Load the best model state before returning, to ensure best validation performance\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return best_val_loss, model\n",
    "\n",
    "\n",
    "\n",
    "def run_cross_val(trial, model_class, X1, y, X2=None, criterion=torch.nn.MSELoss()):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    hidden_size = trial.suggest_categorical(\"hidden_size\", [16, 32, 64, 128, 256])\n",
    "    num_layers = trial.suggest_categorical(\"num_layers\", [1, 2, 3])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    fold_results, test_mae_per_fold, test_rmse_per_fold, test_losses = [], [], [], []  # Added test_losses to track test loss\n",
    "    best_fold_model = None  # Initialize the best model to track it for return\n",
    "    best_val_loss = float('inf')  # Track the best validation loss across folds\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(BlockingTimeSeriesSplit(n_splits=5).split(X1)):\n",
    "        train_val_split_idx = int(0.8 * len(train_idx))\n",
    "        train_idx, val_index = train_idx[:train_val_split_idx], train_idx[train_val_split_idx:]\n",
    "    \n",
    "        X1_train, X1_val, X1_test, y_train, y_val, y_test = X1[train_idx], X1[val_index], X1[test_idx], y[train_idx], y[val_index], y[test_idx]\n",
    "        \n",
    "        # Prepare train, validation, and test sets\n",
    "        X1_train_scaled, X1_val_scaled, X1_test_scaled = scale_data(X1_train, X1_val, X1_test)\n",
    "\n",
    "        if X2 is not None:\n",
    "            X2_train, X2_val, X2_test = X2[train_idx], X2[val_index], X2[test_idx]\n",
    "\n",
    "            train_loader = DataLoader(TensorDataset(X1_train_scaled, X2_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(TensorDataset(X1_val_scaled, X2_val, y_val), batch_size=len(X1_val_scaled), shuffle=False)\n",
    "            test_loader = DataLoader(TensorDataset(X1_test_scaled, X2_test, y_test), batch_size=len(X1_test_scaled), shuffle=False)\n",
    "            \n",
    "            model = model_class(input_size=X1.shape[2], hidden_size=hidden_size, num_layers=num_layers, output_size=1, auxiliary_input_dim = X2.shape[1]).to(device)\n",
    "            fold_val_loss, model = train_model_dual_input(model, criterion, torch.optim.Adam(model.parameters(), lr=learning_rate), train_loader, val_loader)\n",
    "\n",
    "        else:\n",
    "            train_loader = DataLoader(TensorDataset(X1_train_scaled, y_train), batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(TensorDataset(X1_val_scaled, y_val), batch_size=len(X1_val_scaled), shuffle=False)\n",
    "            test_loader = DataLoader(TensorDataset(X1_test_scaled, y_test), batch_size=len(X1_test_scaled), shuffle=False)\n",
    "            \n",
    "            model = model_class(input_size=X1.shape[2], hidden_size=hidden_size, num_layers=num_layers, output_size=1).to(device)\n",
    "            fold_val_loss, model = train_model_single_input(model, criterion, torch.optim.Adam(model.parameters(), lr=learning_rate), train_loader, val_loader)\n",
    "        \n",
    "        fold_results.append(fold_val_loss)\n",
    "\n",
    "        if fold_val_loss < best_val_loss:\n",
    "            best_val_loss = fold_val_loss\n",
    "            best_fold_model = model \n",
    "\n",
    "        # Test evaluation\n",
    "        all_predictions, all_true_values = [], []\n",
    "        model.eval()\n",
    "        test_loss = 0  # Initialize test loss for this fold\n",
    "        with torch.no_grad():\n",
    "            for batch_data in test_loader:\n",
    "                if X2 is not None:\n",
    "                    batch_X1, batch_X2, batch_y = batch_data[0].to(device), batch_data[1].to(device), batch_data[2].to(device)\n",
    "                    predictions = model(batch_X1, batch_X2)\n",
    "                else:\n",
    "                    batch_X1, batch_y = batch_data[0].to(device), batch_data[1].to(device)\n",
    "                    predictions = model(batch_X1)\n",
    "                test_loss += criterion(predictions, batch_y).item()  # Accumulate test loss\n",
    "                all_predictions.append(predictions)\n",
    "                all_true_values.append(batch_y)\n",
    "\n",
    "        # Calculate average test loss for the fold\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)  # Append to test_losses for this fold\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_true_values = torch.cat(all_true_values)\n",
    "        mae, rmse = evaluate_metrics(all_predictions, all_true_values)\n",
    "        test_mae_per_fold.append(mae)\n",
    "        test_rmse_per_fold.append(rmse)\n",
    "\n",
    "    mean_val_loss = np.mean(fold_results)\n",
    "    mean_test_loss = np.mean(test_losses)  # Calculate mean test loss across folds\n",
    "    mean_test_mae, std_test_mae = np.mean(test_mae_per_fold), np.std(test_mae_per_fold)\n",
    "    mean_test_rmse, std_test_rmse = np.mean(test_rmse_per_fold), np.std(test_rmse_per_fold)\n",
    "\n",
    "    # Log results\n",
    "    with open(f\"results_{model_class.__name__}.csv\", mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if file.tell() == 0:\n",
    "            writer.writerow([\n",
    "                \"trial_number\", \"learning_rate\", \"batch_size\", \"hidden_size\", \"num_layers\",\n",
    "                *[\"fold_{}_val_loss\".format(i+1) for i in range(5)], \"avg_validation_loss\",\n",
    "                \"avg_test_loss\",  # Log average test loss\n",
    "                \"test_mae_mean\", \"test_mae_std\", \"test_rmse_mean\", \"test_rmse_std\"\n",
    "            ])\n",
    "        writer.writerow([\n",
    "            trial.number, learning_rate, batch_size, hidden_size, num_layers,\n",
    "            *fold_results, mean_val_loss, mean_test_loss,  # Include mean test loss in log\n",
    "            mean_test_mae, std_test_mae, mean_test_rmse, std_test_rmse\n",
    "        ])\n",
    "        file.flush()\n",
    "\n",
    "    return mean_val_loss, mean_test_loss, best_fold_model # Return mean test loss along with validation loss\n",
    "\n",
    "\n",
    "def run_optuna_study(objective_func, model_class, X1, y, file_name: str, n_trials: int = 100, X2=None):\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=seed))\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = f\"{model_class.__name__}_best_model.pth\"  # Fixed path for the best model\n",
    "\n",
    "    def objective(trial):\n",
    "        criterion = torch.nn.MSELoss()  # Define the criterion here\n",
    "        trial_val_loss, _, model = objective_func(trial, model_class, X1, y, X2, criterion)  # Pass criterion\n",
    "\n",
    "        # Save model if it's the best one so far\n",
    "        nonlocal best_val_loss, best_model_path\n",
    "        if trial_val_loss < best_val_loss:\n",
    "            best_val_loss = trial_val_loss\n",
    "\n",
    "            # Remove previous best model file if it exists\n",
    "            if os.path.exists(best_model_path):\n",
    "                os.remove(best_model_path)\n",
    "\n",
    "            # Save the new best model with a fixed path\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Saved new best model with validation loss {best_val_loss} at {best_model_path}\")\n",
    "\n",
    "        return trial_val_loss\n",
    "\n",
    "    # Let Optuna manage trials and pass them to the objective function\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    joblib.dump(study, file_name)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best cross-validated validation loss:\", study.best_value)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1Input\n",
    "run_optuna_study(run_cross_val, LSTMX1Input, X1, y, \"LSTMX1Input_study.pkl\", n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1_X2BeforeLSTM\n",
    "run_optuna_study(run_cross_val, LSTMX1_X2BeforeLSTM, X1, y, \"LSTMX1_X2BeforeLSTM_study.pkl\", n_trials=10, X2=X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1_X2Masking\n",
    "run_optuna_study(run_cross_val, LSTMX1_X2Masking, X1, y, \"LSTMX1_X2Masking_study.pkl\", n_trials=10, X2=masking_X1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_motifpredenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
