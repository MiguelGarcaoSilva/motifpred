{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "#np.random.seed(1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_dataset(data, variable_indexes, lookback_period, step, forecast_period, motif_indexes):\n",
    "    X1, X2, y = [], [], []  # X1: data, X2: indexes of the motifs, y: distance to the next motif\n",
    "    \n",
    "    for idx in range(len(data[0]) - lookback_period - 1):\n",
    "        if idx % step != 0:\n",
    "            continue\n",
    "\n",
    "        window_end_idx = idx + lookback_period\n",
    "        forecast_period_end = window_end_idx + forecast_period\n",
    "\n",
    "        # If there are no more matches after the window, break\n",
    "        if not any([window_end_idx < motif_idx for motif_idx in motif_indexes]):\n",
    "            break\n",
    "\n",
    "        # Motif indexes in window, relative to the start of the window\n",
    "        motif_indexes_in_window = [motif_idx - idx for motif_idx in motif_indexes if idx <= motif_idx <= window_end_idx]\n",
    "        motif_indexes_in_forecast_period = [motif_idx for motif_idx in motif_indexes if window_end_idx < motif_idx <= forecast_period_end]\n",
    "\n",
    "        if motif_indexes_in_forecast_period:\n",
    "            next_match_in_forecast_period = motif_indexes_in_forecast_period[0]\n",
    "        else:\n",
    "            next_match_in_forecast_period = -1  # No match in the forecast period but exists in the future\n",
    "\n",
    "        # Get the data window and transpose to (lookback_period, num_features)\n",
    "        data_window = data[variable_indexes, idx:window_end_idx].T\n",
    "\n",
    "        # Calculate `y`\n",
    "        data_y = -1\n",
    "        if next_match_in_forecast_period != -1:\n",
    "            # Index of the next match relative to the end of the window\n",
    "            data_y = next_match_in_forecast_period - window_end_idx\n",
    "        \n",
    "        # Append to lists\n",
    "        X1.append(torch.tensor(data_window, dtype=torch.float32))  # Now with shape (lookback_period, num_features)\n",
    "        X2.append(torch.tensor(motif_indexes_in_window, dtype=torch.long)) \n",
    "        y.append(data_y) \n",
    "\n",
    "    # Pad X2 sequences to have the same length\n",
    "    X2_padded = pad_sequence(X2, batch_first=True, padding_value=-1)\n",
    "    \n",
    "    # Convert lists to torch tensors\n",
    "    X1 = torch.stack(X1)  # Final shape: (num_samples, lookback_period, num_features)\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return X1, X2_padded, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "#fixed frequency repetitions\n",
    "n = 1000\n",
    "k = 3\n",
    "variable_indexes = range(k)\n",
    "\n",
    "data_scenario1 = np.genfromtxt(\"../data/syntheticdata/scenario1_k=3.csv\", delimiter=\",\")\n",
    "data_scenario1 = data_scenario1.astype(int)\n",
    "data_scenario1 = data_scenario1.reshape((k, n))\n",
    "\n",
    "motif_indexes_scenario1 = np.genfromtxt(\"../data/syntheticdata/motif_indexes_scenario1_k=3.csv\", delimiter=\",\")\n",
    "motif_indexes_scenario1= motif_indexes_scenario1.astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(k, 1, sharex=True)\n",
    "for i in range(k):\n",
    "    axs[i].plot(data_scenario1[i], linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_period = 100 #window size\n",
    "step = 1 #step size for the sliding window\n",
    "forecast_period = 50 #forward window size\n",
    "\n",
    "#x1: past window + masking, x2: indexes of the motif in the window,  y: next relative index of the motif\n",
    "X1, X2, y = create_dataset(data_scenario1, variable_indexes, lookback_period, step, forecast_period, motif_indexes_scenario1)\n",
    "\n",
    "#print one input output pair\n",
    "print(X1[0])\n",
    "print(X2[0])\n",
    "print(y[0])\n",
    "\n",
    "# X1, X2, and y are now PyTorch tensors\n",
    "print(\"X1 shape:\", X1.shape)  # Expected shape: (num_samples, lookback_period, num_features)\n",
    "print(\"X2 shape:\", X2.shape)  # Expected shape: (num_samples, max_motif_length_in_window)\n",
    "print(\"y shape:\", y.shape)    # Expected shape: (num_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "  def __init__(self, n_splits):\n",
    "      self.n_splits = n_splits\n",
    "\n",
    "  def get_n_splits(self, X, y, groups):\n",
    "      return self.n_splits\n",
    "\n",
    "  def split(self, X, y=None, groups=None):\n",
    "      n_samples = len(X)\n",
    "      k_fold_size = n_samples // self.n_splits\n",
    "      indices = np.arange(n_samples)\n",
    "\n",
    "      margin = 0\n",
    "      for i in range(self.n_splits):\n",
    "          start = i * k_fold_size\n",
    "          stop = start + k_fold_size\n",
    "          mid = int(0.8 * (stop - start)) + start\n",
    "          yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X1, X2, y):\n",
    "        self.X1 = X1  # Time series data\n",
    "        self.X2 = X2  # Motif indexes data\n",
    "        self.y = y    # Target values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: check if this is how i want to deal with the x2 data\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer for processing X1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(hidden_size + X2.shape[1], output_size)\n",
    "        \n",
    "    def forward(self, X1, X2):\n",
    "        batch_size = X1.size(0)\n",
    "        # Initialize hidden and cell states for LSTM\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(X1.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(X1.device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(X1, (h0, c0))\n",
    "\n",
    "        # Get the last time step's output\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Concatenate with X2 (motif indexes)\n",
    "        out = torch.cat((out, X2.float()), dim=1)  # Concatenate along the feature dimension\n",
    "\n",
    "        # Pass through the final fully connected layer\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data n=1000, k=3, split 5-fold, 80% train, 20% test\n",
    "# train CV1 0-141 test CV1 142-177\n",
    "# train CV2 178-319 test CV2 320-355\n",
    "# train CV3 356-497 test CV3 498-533\n",
    "# train CV4 534-675 test CV4 676-711\n",
    "# train CV5 712-853 test CV5 854-889\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Model parameters\n",
    "input_size = X1.shape[2]  # Number of features in X1\n",
    "hidden_size = 100          # LSTM hidden layer size\n",
    "num_layers = 1            # Number of LSTM layers\n",
    "output_size = 1           # Since we are predicting a single value\n",
    "\n",
    "\n",
    "# Placeholder for results\n",
    "fold_results = []\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(BlockingTimeSeriesSplit(n_splits=5).split(X1)):\n",
    "    # Create train and validation sets for each split\n",
    "    X1_train, X1_val = X1[train_idx], X1[val_idx]\n",
    "    X2_train, X2_val = X2[train_idx], X2[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Initialize a new scaler for each fold (only for X1)\n",
    "    scaler_X1 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Fit the scaler on the training data\n",
    "    X1_train_reshaped = X1_train.view(-1, X1_train.shape[-1])  # Flatten for scaling\n",
    "    X1_train_scaled = scaler_X1.fit_transform(X1_train_reshaped)\n",
    "    X1_train = torch.tensor(X1_train_scaled, dtype=torch.float32).view(X1_train.shape)\n",
    "\n",
    "    # Transform the validation set using the training scaler\n",
    "    X1_val_reshaped = X1_val.view(-1, X1_val.shape[-1])  # Flatten for scaling\n",
    "    X1_val_scaled = scaler_X1.transform(X1_val_reshaped)\n",
    "    X1_val = torch.tensor(X1_val_scaled, dtype=torch.float32).view(X1_val.shape)\n",
    "    \n",
    "    # Create DataLoader for train and validation sets\n",
    "    train_dataset = TimeSeriesDataset(X1_train, X2_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X1_val, X2_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    #TODO: check train and val loader shape, mismatch\n",
    "\n",
    "    # Initialize model, loss function, and optimizer for each fold\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop for the fold\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for batch_X1, batch_X2, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(batch_X1, batch_X2)\n",
    "            print(outputs.shape)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            print(loss.shape)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        if epoch % 10 + 1 == 0:\n",
    "            print(f'Fold {fold+1}, Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X1, batch_X2, batch_y in val_loader:\n",
    "            outputs = model(batch_X1, batch_X2)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Log validation loss for the fold\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    fold_results.append(avg_val_loss)\n",
    "    print(f'Fold {fold+1} Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(fold_results)\n",
    "# Display overall cross-validation performance\n",
    "print(f'Mean Validation Loss across folds: {np.mean(fold_results):.4f}')\n",
    "print(f'Standard Deviation of Validation Loss across folds: {np.std(fold_results):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "\n",
    "# Model parameters\n",
    "input_size = X1.shape[2]  # Number of features in X1\n",
    "output_size = 1           \n",
    "\n",
    "# Open the CSV file to log Optuna results\n",
    "with open(\"optuna_tuning_results.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header, including a separate column for each fold's loss\n",
    "    writer.writerow([\"trial_number\", \"learning_rate\", \"batch_size\", \"hidden_size\", \"num_layers\", \"fold_1_loss\", \"fold_2_loss\", \"fold_3_loss\", \"fold_4_loss\", \"fold_5_loss\", \"avg_validation_loss\"])\n",
    "\n",
    "    # Define the Optuna objective function\n",
    "    def objective(trial):\n",
    "        # Define hyperparameters to tune\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 1, 1)\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 16, 128)\n",
    "\n",
    "        # Placeholder for fold validation losses\n",
    "        fold_results = []\n",
    "\n",
    "        # Cross-validation with BlockingTimeSeriesSplit\n",
    "        for fold, (train_idx, val_idx) in enumerate(BlockingTimeSeriesSplit(n_splits=5).split(X1)):\n",
    "            # Split train and validation sets for each fold\n",
    "            X1_train, X1_val = X1[train_idx], X1[val_idx]\n",
    "            X2_train, X2_val = X2[train_idx], X2[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            # Scale the training and validation data for X1\n",
    "            scaler_X1 = MinMaxScaler(feature_range=(0, 1))\n",
    "            X1_train_scaled = scaler_X1.fit_transform(X1_train.view(-1, X1_train.shape[-1]))\n",
    "            X1_train = torch.tensor(X1_train_scaled, dtype=torch.float32).view(X1_train.shape)\n",
    "            X1_val_scaled = scaler_X1.transform(X1_val.view(-1, X1_val.shape[-1]))\n",
    "            X1_val = torch.tensor(X1_val_scaled, dtype=torch.float32).view(X1_val.shape)\n",
    "            \n",
    "            # Create DataLoader for train and validation sets\n",
    "            train_dataset = TimeSeriesDataset(X1_train, X2_train, y_train)\n",
    "            val_dataset = TimeSeriesDataset(X1_val, X2_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize model, loss function, and optimizer for each fold\n",
    "            model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            \n",
    "            # Training loop for the fold\n",
    "            num_epochs = 200  \n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                epoch_loss = 0\n",
    "\n",
    "                for batch_X1, batch_X2, batch_y in train_loader:\n",
    "                    # Forward pass\n",
    "                    outputs = model(batch_X1, batch_X2)\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X1, batch_X2, batch_y in val_loader:\n",
    "                    outputs = model(batch_X1, batch_X2)\n",
    "                    loss = criterion(outputs.squeeze(), batch_y)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            # Calculate and store the average validation loss for this fold\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            fold_results.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate the mean validation loss across all folds for this trial\n",
    "        mean_val_loss = np.mean(fold_results)\n",
    "\n",
    "        # Write the trial's results to the CSV file, including each fold's loss\n",
    "        writer.writerow([trial.number, learning_rate, batch_size, hidden_size, num_layers] + fold_results + [mean_val_loss])\n",
    "        file.flush()  # Ensure each trial result is written immediately\n",
    "\n",
    "        # Return the mean validation loss across folds for this trial\n",
    "        return mean_val_loss\n",
    "\n",
    "    # Run the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)  # Adjust n_trials as desired\n",
    "\n",
    "    # Print the best hyperparameters and validation loss\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best cross-validated validation loss:\", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_motifpredenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
