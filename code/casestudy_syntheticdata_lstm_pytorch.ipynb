{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "results_dir = '../results/all_variables'\n",
    "images_dir = '../images/all_variables'\n",
    "data_dir = '../data/syntheticdata/all_variables'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from utils.train_pipeline import ModelTrainingPipeline\n",
    "\n",
    "seed = 1729\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ModelTrainingPipeline.set_seed(seed)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "n = 100000 #number of data points\n",
    "k = 3 #number of variables\n",
    "p = 5 # pattern length\n",
    "variable_indexes = np.arange(k)\n",
    "#variables_pattern = [0,2]\n",
    "\n",
    "dataset_path = os.path.join(data_dir, \"n={}_k={}_p={}_min_step={}_max_step={}.csv\".format(n, k, p, 5, 45))\n",
    "motif_indexes_path = os.path.join(data_dir, \"motif_indexes_n={}_k={}_p={}_min_step={}_max_step={}.csv\".format(n, k, p, 5, 45))\n",
    "data = np.genfromtxt(dataset_path, delimiter=\",\").astype(int).reshape((k, n))\n",
    "motif_indexes = np.genfromtxt(motif_indexes_path, delimiter=\",\").astype(int)\n",
    "\n",
    "print(motif_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.timeseries_split import BlockingTimeSeriesSplit\n",
    "\n",
    "#create index  \n",
    "indexes = np.arange(len(data[0]))\n",
    "\n",
    "#split data\n",
    "tscv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "# Create the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(indexes)):\n",
    "    # Plot train and test indices\n",
    "    ax.plot(train_index, np.zeros_like(train_index) + i, 'o', color='lightblue')\n",
    "    ax.plot(test_index, np.zeros_like(test_index) + i, 'o', color='red')\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    \n",
    "\n",
    "ax.set_yticks(np.arange(5), [\"Fold {}\".format(i) for i in range(1, 6)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import create_dataset\n",
    "\n",
    "lookback_period = 100 #window size\n",
    "step = 5 #step size for the sliding window\n",
    "forecast_period = 50 #forward window size\n",
    "\n",
    "#x1: past window, x2: indexes of the motif in the window,  y: next relative index of the motif\n",
    "X1, X2, y = create_dataset(data, variable_indexes, lookback_period, step, forecast_period, motif_indexes)\n",
    "\n",
    "# X1, X2, and y are now PyTorch tensors\n",
    "print(\"X1 shape:\", X1.shape)  # Expected shape: (num_samples, lookback_period, num_features)\n",
    "print(\"X2 shape:\", X2.shape)  # Expected shape: (num_samples, max_motif_length_in_window)\n",
    "print(\"y shape:\", y.shape)    # Expected shape: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1\n",
    "from utils.train_pipeline import EarlyStopper, ModelTrainingPipeline, run_optuna_study\n",
    "from utils.utils import print_study_results, plot_best_model_results\n",
    "\n",
    "\n",
    "n_trials = 3\n",
    "num_epochs = 5\n",
    "model_type = \"LSTM\"\n",
    "model_name = \"LSTM_X1\"\n",
    "\n",
    "suggestion_dict = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"args\": [1e-5, 1e-3], \n",
    "        \"kwargs\": {\"log\": True} \n",
    "    },\n",
    "    \"hidden_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128, 256]] \n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[1, 2, 3]]  \n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128]] \n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_keys = [\"hidden_size\", \"num_layers\"]\n",
    "\n",
    "\n",
    "result_dir = os.path.join(results_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "os.makedirs(result_dir, exist_ok=True)  \n",
    "\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=1e-5, min_epochs=100)\n",
    "pipeline = ModelTrainingPipeline(device=device, early_stopper=early_stopper)\n",
    "\n",
    "run_optuna_study(pipeline.run_cross_val, LSTMX1, model_type, suggestion_dict, model_params_keys, seed, X1, None, y, result_dir, n_trials=n_trials, num_epochs=num_epochs)\n",
    "\n",
    "study = joblib.load(os.path.join(result_dir, \"study.pkl\"))\n",
    "print_study_results(study)\n",
    "plot_best_model_results(study.trials_dataframe(), save_path=os.path.join(images_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs_losses.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import plot_preds_vs_truevalues\n",
    "from utils.train_pipeline import get_preds_best_config\n",
    "\n",
    "epochs_train_losses, epochs_val_losses, all_predictions, all_true_values = get_preds_best_config(study, pipeline, LSTMX1, model_type, model_params_keys, num_epochs =num_epochs, seed=seed, X1=X1, X2=None, y=y)\n",
    "\n",
    "# Plot the train and validation losses for each fold\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 5), sharey=True)\n",
    "for i in range(5):\n",
    "    axes[i].plot(epochs_train_losses[i], label=\"Train Loss\")\n",
    "    axes[i].plot(epochs_val_losses[i], label=\"Validation Loss\")\n",
    "    axes[i].set_title(f\"Fold {i + 1}\")\n",
    "    axes[i].set_xlabel(\"Epoch\")\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel(\"Loss\")\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the predictions vs true values for each fold\n",
    "for fold in range(5):\n",
    "    plot_preds_vs_truevalues(np.ravel(all_true_values[fold]), np.ravel(all_predictions[fold]), fold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1_X2BeforeLSTM\n",
    "from utils.train_pipeline import EarlyStopper, ModelTrainingPipeline, run_optuna_study \n",
    "\n",
    "n_trials = 5\n",
    "num_epochs = 5\n",
    "model_type = \"LSTM\"\n",
    "model_name = \"LSTM_X1_X2BeforeLSTM\"\n",
    "\n",
    "suggestion_dict = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"args\": [1e-5, 1e-3], \n",
    "        \"kwargs\": {\"log\": True} \n",
    "    },\n",
    "    \"hidden_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128, 256]] \n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[1, 2, 3]]  \n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128]] \n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_keys = [\"hidden_size\", \"num_layers\"]\n",
    "\n",
    "result_dir = os.path.join(results_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=1e-5, min_epochs=100)\n",
    "pipeline = ModelTrainingPipeline(device=device, early_stopper=early_stopper)\n",
    "\n",
    "run_optuna_study(pipeline.run_cross_val, LSTMX1_X2BeforeLSTM, model_type, suggestion_dict, model_params_keys, seed, X1, X2, y, result_dir, n_trials=n_trials, num_epochs=num_epochs)\n",
    "\n",
    "study = joblib.load(os.path.join(result_dir, \"study.pkl\"))\n",
    "print_study_results(study)\n",
    "plot_best_model_results(study.trials_dataframe(), save_path=os.path.join(images_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs_losses.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1_X2AfterLSTM\n",
    "from utils.train_pipeline import EarlyStopper, ModelTrainingPipeline, run_optuna_study\n",
    "\n",
    "n_trials = 5\n",
    "num_epochs = 5\n",
    "model_type = \"LSTM\"\n",
    "model_name = \"LSTMX1_X2AfterLSTM\"\n",
    "\n",
    "suggestion_dict = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"args\": [1e-5, 1e-3], \n",
    "        \"kwargs\": {\"log\": True} \n",
    "    },\n",
    "    \"hidden_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128, 256]] \n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[1, 2, 3]]  \n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128]] \n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_keys = [\"hidden_size\", \"num_layers\"]\n",
    "\n",
    "result_dir = os.path.join(results_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=1e-5, min_epochs=100)\n",
    "pipeline = ModelTrainingPipeline(device=device, early_stopper=early_stopper)\n",
    "\n",
    "run_optuna_study(pipeline.run_cross_val, LSTMX1_X2AfterLSTM, model_type, suggestion_dict, model_params_keys, seed, X1, X2, y, result_dir, n_trials=n_trials, num_epochs=num_epochs)\n",
    "\n",
    "study = joblib.load(os.path.join(result_dir, \"study.pkl\"))\n",
    "print_study_results(study)\n",
    "plot_best_model_results(study.trials_dataframe(), save_path=os.path.join(images_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs_losses.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMX1_X2Masking\n",
    "\n",
    "n_trials = 5\n",
    "num_epochs = 5\n",
    "model_type = \"LSTM\"\n",
    "model_name = \"LSTMX1_X2AfterLSTM\"\n",
    "\n",
    "suggestion_dict = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"args\": [1e-5, 1e-3], \n",
    "        \"kwargs\": {\"log\": True} \n",
    "    },\n",
    "    \"hidden_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128, 256]] \n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[1, 2, 3]]  \n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128]] \n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_keys = [\"hidden_size\", \"num_layers\"]\n",
    "\n",
    "#X1 shape is (num_samples, lookback_period)\n",
    "masking_X1 = np.zeros((X1.shape[0], X1.shape[1])) \n",
    "\n",
    "for i, obs_motif_indexes in enumerate(X2):\n",
    "    for j, idx in enumerate(obs_motif_indexes):\n",
    "        masking_X1[i, idx.item():idx.item()+p] = 1\n",
    "\n",
    "masking_X1 = torch.tensor(masking_X1, dtype=torch.float32)\n",
    "\n",
    "result_dir = os.path.join(results_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=1e-5, min_epochs=100)\n",
    "pipeline = ModelTrainingPipeline(device=device, early_stopper=early_stopper)\n",
    "\n",
    "run_optuna_study(pipeline.run_cross_val, LSTMX1_X2Masking, model_type, suggestion_dict, model_params_keys, seed, X1, masking_X1, y, result_dir, n_trials=n_trials, num_epochs=num_epochs)\n",
    "\n",
    "study = joblib.load(os.path.join(result_dir, \"study.pkl\"))\n",
    "print_study_results(study)\n",
    "plot_best_model_results(study.trials_dataframe(), save_path=os.path.join(images_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs_losses.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm_pytorch import LSTMAttentionX1\n",
    "\n",
    "n_trials = 5\n",
    "num_epochs = 5\n",
    "model_type = \"LSTM\"\n",
    "model_name = \"LSTMAttentionX1\"\n",
    "\n",
    "suggestion_dict = {\n",
    "    \"learning_rate\": {\n",
    "        \"type\": \"float\",\n",
    "        \"args\": [1e-5, 1e-3], \n",
    "        \"kwargs\": {\"log\": True} \n",
    "    },\n",
    "    \"hidden_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128, 256]] \n",
    "    },\n",
    "    \"num_layers\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[1, 2, 3]]  \n",
    "    },\n",
    "    \"batch_size\": {\n",
    "        \"type\": \"categorical\",\n",
    "        \"args\": [[16, 32, 64, 128]] \n",
    "    }\n",
    "}\n",
    "\n",
    "model_params_keys = [\"hidden_size\", \"num_layers\"]\n",
    "\n",
    "result_dir = os.path.join(results_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=1e-5, min_epochs=100)\n",
    "pipeline = ModelTrainingPipeline(device=device, early_stopper=early_stopper)\n",
    "\n",
    "run_optuna_study(pipeline.run_cross_val, LSTMAttentionX1, model_type, suggestion_dict, model_params_keys, seed, X1, None, y, result_dir, n_trials=n_trials, num_epochs=num_epochs)\n",
    "\n",
    "study = joblib.load(os.path.join(result_dir, \"study.pkl\"))\n",
    "print_study_results(study)\n",
    "plot_best_model_results(study.trials_dataframe(), save_path=os.path.join(images_dir, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs_losses.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_motifpredenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
