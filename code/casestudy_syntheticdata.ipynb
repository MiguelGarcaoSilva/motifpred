{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "keras.utils.set_random_seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project = \"motifpred\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create synthetic time series random uniform integers\n",
    "n = 1000\n",
    "data = np.random.randint(1, 6, n)\n",
    "\n",
    "#select random timewindows of length 5 without overlapping\n",
    "motif_indexes = []\n",
    "motif_pattern = [1,1,1,1,1]\n",
    "p = len(motif_pattern)\n",
    "\n",
    "max_interval = n//20\n",
    "print(max_interval)\n",
    "last_index = 0\n",
    "while True:\n",
    "    index_interval = np.random.randint(p+4, p + max_interval)\n",
    "    if last_index + index_interval + p > n:\n",
    "        break\n",
    "    last_index = last_index + index_interval\n",
    "    motif_indexes.append(last_index)\n",
    "\n",
    "motif_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the values of the time series in the selected timewindows to the motif pattern\n",
    "for idx in motif_indexes:\n",
    "    data[idx:idx+p] = motif_pattern\n",
    "\n",
    "#introduce clue in the data\n",
    "for idx in motif_indexes:\n",
    "    data[idx-5:idx-2] = [1,2,3]\n",
    "\n",
    "#plot the data and in red the motif pattern\n",
    "plt.plot(data)\n",
    "for idx in motif_indexes:\n",
    "    plt.plot(range(idx, idx+p), motif_pattern, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def create_dataset(data, past_window, step, forward_window, motif_indexes):\n",
    "    X1, X2, y1, y = list(), list(), list(), list()\n",
    "    for idx in range(len(data) - past_window - 1):\n",
    "        if idx % step != 0:\n",
    "            continue\n",
    "    \n",
    "        next_matches = [motif_idx for motif_idx in motif_indexes if motif_idx > idx + past_window]\n",
    "        if not next_matches:\n",
    "            continue #no match\n",
    "        next_match = next_matches[0]\n",
    "        if next_match > idx + past_window + forward_window:\n",
    "            next_match = -1 #no match in forward window\n",
    "\n",
    "        data_x1 = data[idx:idx+past_window]\n",
    "        data_x2 = [motif_idx for motif_idx in motif_indexes if motif_idx <= idx+past_window]\n",
    "        data_y1 = data[idx+past_window]\n",
    "        data_y = next_match\n",
    "        X1.append(data_x1)\n",
    "        X2.append(data_x2)\n",
    "        y1.append(data_y1)\n",
    "        y.append(data_y) \n",
    "\n",
    "    X2 = pad_sequences(X2, padding='post', value=-1, dtype=int)\n",
    "    return np.array(X1), np.array(X2), np.array(y1), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_window = 100 #window size\n",
    "step = 1 #step size for the sliding window\n",
    "forward_window = 50 #\n",
    "X1, X2, y1, y  = create_dataset(data, past_window=past_window, step=step, forward_window=forward_window, motif_indexes=motif_indexes)\n",
    "print(X1[0], X2[0], y1[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input to be [samples, time steps, features]\n",
    "X1 = np.reshape(X1, (X1.shape[0],  X1.shape[1], 1))\n",
    "X2 = np.reshape(X2, (X2.shape[0],  X2.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "  def __init__(self, n_splits):\n",
    "      self.n_splits = n_splits\n",
    "\n",
    "  def get_n_splits(self, X, y, groups):\n",
    "      return self.n_splits\n",
    "\n",
    "  def split(self, X, y=None, groups=None):\n",
    "      n_samples = len(X)\n",
    "      k_fold_size = n_samples // self.n_splits\n",
    "      indices = np.arange(n_samples)\n",
    "\n",
    "      margin = 0\n",
    "      for i in range(self.n_splits):\n",
    "          start = i * k_fold_size\n",
    "          stop = start + k_fold_size\n",
    "          mid = int(0.8 * (stop - start)) + start\n",
    "          yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train lstm\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, Dense, Masking\n",
    "from keras import Input\n",
    "\n",
    "def create_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X2.shape[1], X2.shape[2])))\n",
    "    model.add(Masking(mask_value=-1))\n",
    "    units = hp.Int('units', min_value=10, max_value=50, step=10)\n",
    "    model.add(LSTM(units=units, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=hp_learning_rate), metrics=['mae', 'root_mean_squared_error', 'r2_score'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import keras_tuner as kt\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "\n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        original_callbacks = kwargs.pop(\"callbacks\", [])\n",
    "        verbose = kwargs.pop(\"verbose\", 0)\n",
    "\n",
    "        metrics = collections.defaultdict(list)\n",
    "        batch_size = trial.hyperparameters.Int('batch_size', 8, 32, step=8)\n",
    "        epochs = trial.hyperparameters.Int('epochs', 400, 400, step=100)\n",
    "        cv = BlockingTimeSeriesSplit(n_splits=5)\n",
    "        for train_indices, test_indices in cv.split(x):\n",
    "            X_train, X_test = x[train_indices], x[test_indices]\n",
    "            y_train, y_test = y[train_indices], y[test_indices]\n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=original_callbacks, verbose=verbose)\n",
    "            val_loss, val_mae, val_rmse, val_r2 = model.evaluate(X_test, y_test) \n",
    "            metrics[\"val_loss\"].append(val_loss)\n",
    "            metrics[\"val_mae\"].append(val_mae)\n",
    "            metrics[\"val_rmse\"].append(val_rmse)\n",
    "            metrics[\"val_r2\"].append(val_r2)\n",
    "            print(f\"val_loss: {val_loss}, val_mae: {val_mae}, val_rmse: {val_rmse}, val_r2: {val_r2}\")\n",
    "\n",
    "        self.save_model(trial, model)\n",
    "        return {name: np.mean(values) for name, values in metrics.items()}\n",
    "    \n",
    "    def save_model(self, trial, model):\n",
    "        fname = os.path.join(self.get_trial_dir(trial.trial_id), \"model.pickle\")\n",
    "        with open(fname, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    def load_model(self, trial):\n",
    "        fname = os.path.join(self.get_trial_dir(trial.trial_id), \"model.pickle\")\n",
    "        with open(fname, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "tuner = CVTuner(\n",
    "  hypermodel=create_model,\n",
    "  oracle=kt.oracles.GridSearchOracle(\n",
    "    objective='val_loss',\n",
    "    max_trials=None))\n",
    "\n",
    "tuner.search(\n",
    "        x=X2,\n",
    "        y=y,\n",
    "        verbose=2,\n",
    "        callbacks=[WandbMetricsLogger(log_freq=5)],\n",
    "    )\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = BlockingTimeSeriesSplit(5)\n",
    "scores = {\"val_loss\": [], \"val_mae\": [], \"val_rmse\": [], \"val_r2\": []}\n",
    "observed = []\n",
    "predictions = []\n",
    "for train_indices, test_indices in cv.split(X2):\n",
    "    val_loss, val_mae, val_rmse, val_r2 = best_model.evaluate(X2[test_indices], y[test_indices])\n",
    "    print(val_loss)\n",
    "    observed.extend(y[test_indices])\n",
    "    predictions.extend(best_model.predict(X2[test_indices]))\n",
    "    scores[\"val_loss\"].append(val_loss)\n",
    "    scores[\"val_mae\"].append(val_mae)\n",
    "    scores[\"val_rmse\"].append(val_rmse)\n",
    "    scores[\"val_r2\"].append(val_r2)\n",
    "\n",
    "#boxplot of the scores\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.boxplot(data=pd.DataFrame(scores[\"val_mae\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_embeddinglstm(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, activation='tanh', return_sequences=False))\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.01,0.001])\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=hp_learning_rate), metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "tuner= kt.RandomSearch(\n",
    "        create_model_embeddinglstm,\n",
    "        objective='mae',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=3,\n",
    "        project_name = 'embeddinglstm'\n",
    "        )\n",
    "\n",
    "tuner.search(\n",
    "        x=X1,\n",
    "        y=y1,\n",
    "        epochs=10,\n",
    "        batch_size=64\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y1_train_pred = best_model.predict(X1_train)\n",
    "mse = mean_squared_error(y1_train, y1_train_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y1_train, y1_train_pred)\n",
    "r2 = r2_score(y1_train, y1_train_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2: {r2}')\n",
    "\n",
    "plt.plot(y1_train, label='True')\n",
    "plt.plot(y1_train_pred, label='Predicted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the embedding layer\n",
    "embeddings = best_model.layers[1].get_weights()[0]\n",
    "#embeddings to 1d array\n",
    "embeddings = embeddings.flatten()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the embeddings with the motif indexes\n",
    "X_train = []\n",
    "for x2 in X2_train:\n",
    "    #concat embeddigs with x2\n",
    "    x2 = x2.flatten()\n",
    "    x2 = np.concatenate((embeddings, x2))\n",
    "    X_train.append(x2)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = []\n",
    "for x2 in X2_test:\n",
    "    #concat embeddigs with x2\n",
    "    x2 = x2.flatten()\n",
    "    x2 = np.concatenate((embeddings, x2))\n",
    "    X_test.append(x2)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_train_pred)\n",
    "mae = mean_absolute_error(y_train, y_train_pred)\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2: {r2}')\n",
    "\n",
    "plt.plot(y_train, label='True')\n",
    "plt.plot(y_train_pred, label='Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2: {r2}')\n",
    "\n",
    "plt.plot(y_test, label='True')\n",
    "plt.plot(y_pred, label='Predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the embeddings with the motif indexes and the motif pattern\n",
    "X_train = []\n",
    "for x2 in X2_train:\n",
    "    #concat embeddigs with x2\n",
    "    x2 = x2.flatten()\n",
    "    x2 = np.concatenate((embeddings, x2, motif_pattern))\n",
    "    X_train.append(x2)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = []\n",
    "for x2 in X2_test:\n",
    "    #concat embeddigs with x2\n",
    "    x2 = x2.flatten()\n",
    "    x2 = np.concatenate((embeddings, x2, motif_pattern))\n",
    "    X_test.append(x2)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_train.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motifpredenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
