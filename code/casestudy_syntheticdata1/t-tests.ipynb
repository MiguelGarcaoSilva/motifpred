{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved in: /home/mgsilva/motifpred/results/syntheticdata1/variables=[0,2]\n",
      "Images will be saved in: /home/mgsilva/motifpred/images/syntheticdata1/variables=[0,2]\n",
      "Data will be accessed from: /home/mgsilva/motifpred/data/syntheticdata1/variables=[0,2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import optuna\n",
    "import random\n",
    "import joblib\n",
    "from config import RESULTS_DIR, IMAGES_DIR, DATA_DIR, DATASET_PATH, MOTIF_INDEXES_PATH, K, N, P, NORMALIZE_FLAGS\n",
    "\n",
    "print(f\"Results will be saved in: {RESULTS_DIR}\")\n",
    "print(f\"Images will be saved in: {IMAGES_DIR}\")\n",
    "print(f\"Data will be accessed from: {DATA_DIR}\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if '__file__' in globals():\n",
    "    # For standalone scripts\n",
    "    base_dir = os.path.dirname(__file__)\n",
    "else:\n",
    "    # For Jupyter or interactive environments\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory of `utils` to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(base_dir, \"../\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0    10    22 ... 99922 99956 99992]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(DATASET_PATH, delimiter=\",\").astype(int).reshape((K, N))\n",
    "motif_indexes = np.genfromtxt(MOTIF_INDEXES_PATH, delimiter=\",\").astype(int)\n",
    "\n",
    "print(motif_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.5035, 0.9978, 0.3884],\n",
      "        [0.6929, 0.1703, 0.1384],\n",
      "        [0.4759, 0.7481, 0.0361],\n",
      "        [0.5062, 0.8469, 0.2588],\n",
      "        [0.2707, 0.4115, 0.6839]])\n"
     ]
    }
   ],
   "source": [
    "# Import shared setup\n",
    "from utils.setup import seed, device, early_stopper, pipeline, test_tensor\n",
    "\n",
    "# Example usage\n",
    "print(f\"Device: {device}\")\n",
    "test_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_series shape: torch.Size([19979, 100, 3])\n",
      "X_mask shape: torch.Size([19979, 100])\n",
      "X_indices shape: torch.Size([19979, 6, 1])\n",
      "y shape: torch.Size([19979, 1])\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import create_dataset\n",
    "\n",
    "lookback_period = 100 #window size\n",
    "step = 5 #step size for the sliding window\n",
    "forecast_period = 50 #forward window size\n",
    "\n",
    "#X_series: past window, X_indices: indexes of the motif in the window,  y: next relative index of the motif\n",
    "X_series, X_indices, X_mask, y = create_dataset(data, lookback_period, step, forecast_period, motif_indexes, P)\n",
    "\n",
    "# X_series, X2, and y are now PyTorch tensors\n",
    "print(\"X_series shape:\", X_series.shape)  # Expected shape: (num_samples, lookback_period, num_features)\n",
    "print(\"X_mask shape:\", X_mask.shape)  # Expected shape: (num_samples, lookback_period)\n",
    "print(\"X_indices shape:\", X_indices.shape)  # Expected shape: (num_samples, max_motif_length_in_window)\n",
    "print(\"y shape:\", y.shape)    # Expected shape: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model FFNNSeries already exists\n",
      "Model FFNNSeries_Masking already exists\n",
      "Model FFNNIndexes already exists\n",
      "Model LSTMSeries already exists\n",
      "Model LSTMSeries_Masking already exists\n",
      "Model LSTMIndexes already exists\n",
      "Model CNNSeries already exists\n",
      "Model CNNSeries_Masking already exists\n",
      "Model CNNIndexes already exists\n",
      "Model TCNSeries already exists\n",
      "Best hyperparameters: {'learning_rate': 0.0006119580363897532, 'kernel_size': 7, 'receptive_field': 50, 'dropout': 7.786373649315559e-05, 'batch_size': 32, 'block_channels_0': 32, 'block_channels_1': 32, 'block_channels_2': 32, 'block_channels_3': 16}\n",
      "Stopping early: Maximum training time exceeded.\n",
      "Early stopping at epoch 108, with best epoch being 97\n",
      "Stopping early: Maximum training time exceeded.\n",
      "Early stopping at epoch 108, with best epoch being 98\n",
      "Stopping early: Maximum training time exceeded.\n",
      "Early stopping at epoch 102, with best epoch being 84\n",
      "Early stopping at epoch 111, with best epoch being 100\n",
      "Stopping early: Maximum training time exceeded.\n",
      "Early stopping at epoch 106, with best epoch being 89\n",
      "Validation Losses: [0.52301424741745, 0.16917137801647186, 0.3402737081050873, 0.23002223670482635, 0.29180261492729187]\n",
      "Mean validation loss: 0.31085683703422545\n",
      "Test Losses: [1.1792951822280884, 0.16881930828094482, 0.5165011286735535, 0.389586865901947, 0.20316900312900543]\n",
      "Mean test loss: 0.49147429764270784\n",
      "Test MAE: [0.7655880451202393, 0.3091314733028412, 0.5004546046257019, 0.48510536551475525, 0.32712483406066895]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/mgsilva/motifpred/results/syntheticdata1/variables=[0,2]/TCNIndexes_100_trials_500_epochs/study.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 120\u001b[0m\n\u001b[1;32m    109\u001b[0m     X \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_indices}\n\u001b[1;32m    112\u001b[0m model_params_map \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFNN\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_sizes_list\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_sizes_list\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim_feedforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    118\u001b[0m }\n\u001b[0;32m--> 120\u001b[0m \u001b[43mprocess_non_baseline_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mprocess_non_baseline_model\u001b[0;34m(model_type, model_params_keys, input_name, X, normalize_flags, num_epochs, seed, pipeline, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_results_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstudy.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m fold_val_losses, fold_test_losses \u001b[38;5;241m=\u001b[39m get_best_model_results(study)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_motifpredenv/lib/python3.12/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/mgsilva/motifpred/results/syntheticdata1/variables=[0,2]/TCNIndexes_100_trials_500_epochs/study.pkl'"
     ]
    }
   ],
   "source": [
    "from models.ffnn_pytorch import FFNN\n",
    "from models.lstm_pytorch import LSTM\n",
    "from models.cnn_pytorch import CNN\n",
    "from models.tcn_pytorch import TemporalConvNet\n",
    "from models.transformer_pytorch import TimeSeriesTransformer\n",
    "from models.baseline_pytorch import BaselineAverage, BaselineLastDifference\n",
    "from utils.utils import print_study_results, get_best_model_results, plot_best_model_results, plot_preds_vs_truevalues\n",
    "from utils.train_pipeline import get_preds_best_config\n",
    "\n",
    "models = [\"FFNN\", \"LSTM\", \"CNN\", \"TCN\", \"Transformer\", \"Baseline\"]\n",
    "inputs = [\"Series\", \"Series_Masking\", \"Indexes\"]\n",
    "\n",
    "n_trials = 100\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "def process_baseline_model(model_class, input_name, X, normalize_flags, n_trials, num_epochs, pipeline, seed, y):\n",
    "    \"\"\"Process baseline models.\"\"\"\n",
    "    model_name = f\"{model_class.__name__}{input_name}\"\n",
    "    dir = os.path.join(RESULTS_DIR, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "\n",
    "    if os.path.exists(os.path.join(dir, \"best_model_results.csv\")):\n",
    "        print(f\"Model {model_name} already exists\")\n",
    "        return\n",
    "\n",
    "    study = joblib.load(os.path.join(dir, \"study.pkl\"))\n",
    "    fold_val_losses, fold_test_losses = get_best_model_results(study)\n",
    "\n",
    "    (epochs_train_losses, epochs_val_losses, val_losses, test_losses,\n",
    "     test_mae_per_fold, test_rmse_per_fold, all_predictions, all_true_values) = get_preds_best_config(\n",
    "        study, pipeline, model_class, \"Baseline\", [], num_epochs=num_epochs, seed=seed, X=X, y=y, normalize_flags=normalize_flags\n",
    "    )\n",
    "\n",
    "    if not np.allclose(fold_val_losses, val_losses):\n",
    "        raise Exception(\"Best model val losses are not close to val losses\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results = pd.DataFrame({\n",
    "        \"fold\": np.arange(1, 6),\n",
    "        \"val_loss\": fold_val_losses,\n",
    "        \"test_loss\": fold_test_losses,\n",
    "        \"test_mae\": test_mae_per_fold,\n",
    "        \"test_rmse\": test_rmse_per_fold\n",
    "    })\n",
    "    results.to_csv(os.path.join(dir, \"best_model_results.csv\"), index=False, mode='w')\n",
    "\n",
    "\n",
    "def process_non_baseline_model(model_type, model_params_keys, input_name, X, normalize_flags, num_epochs, seed, pipeline, y):\n",
    "    \"\"\"Process non-baseline models.\"\"\"\n",
    "    model_name = f\"{model_type}{input_name}\"\n",
    "    model_results_dir = os.path.join(RESULTS_DIR, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "\n",
    "    if os.path.exists(os.path.join(model_results_dir, \"best_model_results.csv\")):\n",
    "        print(f\"Model {model_name} already exists\")\n",
    "        return\n",
    "\n",
    "    study = joblib.load(os.path.join(model_results_dir, \"study.pkl\"))\n",
    "    fold_val_losses, fold_test_losses = get_best_model_results(study)\n",
    "\n",
    "    if model_type == \"Transformer\":\n",
    "        model = eval(\"TimeSeriesTransformer\")\n",
    "    elif model_type == \"TCN\":\n",
    "        model = eval(\"TemporalConvNet\")\n",
    "    else:\n",
    "        model = eval(model_type)\n",
    "\n",
    "    (epochs_train_losses, epochs_val_losses, val_losses, test_losses,\n",
    "     test_mae_per_fold, test_rmse_per_fold, all_predictions, all_true_values) = get_preds_best_config(\n",
    "        study, pipeline, model, model_type, model_params_keys, num_epochs=num_epochs, seed=seed, X=X, y=y, normalize_flags=normalize_flags\n",
    "    )\n",
    "\n",
    "    #if not np.allclose(fold_val_losses, val_losses, atol=1):\n",
    "    #    raise Exception(\"Best model val losses are not close to val losses\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results = pd.DataFrame({\n",
    "        \"fold\": np.arange(1, 6),\n",
    "        \"val_loss\": fold_val_losses,\n",
    "        \"test_loss\": fold_test_losses,\n",
    "        \"test_mae\": test_mae_per_fold,\n",
    "        \"test_rmse\": test_rmse_per_fold\n",
    "    })\n",
    "    results.to_csv(os.path.join(model_results_dir, \"best_model_results.csv\"), index=False, mode='w')\n",
    "\n",
    "\n",
    "for model_type in models:\n",
    "    for input_name in inputs:\n",
    "        normalize_flags = NORMALIZE_FLAGS\n",
    "        n_trials, num_epochs = (1, 1) if model_type == \"Baseline\" else (n_trials, num_epochs)\n",
    "\n",
    "        if model_type == \"Baseline\":\n",
    "            if input_name != \"Indexes\":\n",
    "                continue\n",
    "\n",
    "            X = {\"X_series\": X_series, \"X_mask\": X_mask, \"X_indices\": X_indices}\n",
    "            normalize_flags = {\"X_series\": True, \"X_mask\": False, \"X_indices\": False}\n",
    "\n",
    "            for model_class in [BaselineAverage, BaselineLastDifference]:\n",
    "                process_baseline_model(\n",
    "                    model_class, input_name, X, normalize_flags, n_trials, num_epochs, pipeline, seed, y\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            if input_name == \"Series\":\n",
    "                X = {\"X_series\": X_series}\n",
    "            elif input_name == \"Series_Masking\":\n",
    "                X = {\"X_series\": X_series, \"X_mask\": X_mask}\n",
    "            else:\n",
    "                X = {\"X_indices\": X_indices}\n",
    "\n",
    "\n",
    "            model_params_map = {\n",
    "                \"FFNN\": [\"hidden_sizes_list\"],\n",
    "                \"LSTM\": [\"hidden_sizes_list\"],\n",
    "                \"CNN\": [\"kernel_size\", \"num_filters_list\", \"pool_size\"],\n",
    "                \"TCN\": [\"kernel_size\", \"num_channels_list\", \"dropout\"],\n",
    "                \"Transformer\": [\"d_model\", \"n_heads\", \"e_layers\", \"dim_feedforward\", \"dropout\"]\n",
    "            }\n",
    "\n",
    "            process_non_baseline_model(\n",
    "                model_type, model_params_map[model_type], input_name, X, normalize_flags, num_epochs, seed, pipeline, y\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Model: FFNNSeries\n",
      "Processing Model: LSTMSeries\n",
      "Processing Model: CNNSeries\n",
      "Processing Model: TCNSeries\n",
      "Results file for TCNSeries not found. Skipping.\n",
      "Processing Model: TransformerSeries\n",
      "          model   input fold       mae       rmse\n",
      "0          FFNN  Series    1  9.389045  11.669245\n",
      "1          FFNN  Series    2  8.793365  10.960507\n",
      "2          FFNN  Series    3  9.180077  11.325398\n",
      "3          FFNN  Series    4  8.737524  11.029864\n",
      "4          FFNN  Series    5  8.585275  10.890684\n",
      "5          LSTM  Series    1  4.258052   7.083573\n",
      "6          LSTM  Series    2  4.590631   7.886975\n",
      "7          LSTM  Series    3  3.035272   6.043316\n",
      "8          LSTM  Series    4  3.031424   6.217641\n",
      "9          LSTM  Series    5  7.226361   9.748582\n",
      "10          CNN  Series    1  6.470577   8.659925\n",
      "11          CNN  Series    2  6.460783   8.527258\n",
      "12          CNN  Series    3  5.694703   7.675818\n",
      "13          CNN  Series    4  6.876115   9.207098\n",
      "14          CNN  Series    5  6.587561   8.773660\n",
      "15  Transformer  Series    1  1.789328   4.256087\n",
      "16  Transformer  Series    2  2.188045   3.820515\n",
      "17  Transformer  Series    3  1.468080   3.266317\n",
      "18  Transformer  Series    4  1.682931   3.875419\n",
      "19  Transformer  Series    5  1.621739   3.732234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61335/153205014.py:76: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "rng = np.random.default_rng()\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "models = [\"FFNN\", \"LSTM\", \"CNN\",\"TCN\", \"Transformer\", \"Baseline\"]\n",
    "inputs = [\"Series\", \"Series_Masking\", \"Indexes\"]\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\"model\", \"input\", \"fold\", \"mae\", \"rmse\"])\n",
    "\n",
    "# Combine each model with each input\n",
    "for model_type in models:\n",
    "    for input_name in inputs:\n",
    "        # Handle baseline-specific logic\n",
    "        if model_type == \"Baseline\":\n",
    "            n_trials, num_epochs = (1, 1)\n",
    "            if input_name != \"Indexes\":\n",
    "                continue\n",
    "            \n",
    "            # Process both BaselineAverage and BaselineLastDifference\n",
    "            baseline_variants = [\"BaselineAverage\", \"BaselineLastDifference\"]\n",
    "            for baseline_type in baseline_variants:\n",
    "                model_name = f\"{baseline_type}{input_name}\"\n",
    "                print(f\"Processing Model: {model_name}\")\n",
    "\n",
    "                # Construct the results directory path\n",
    "                model_results_dir = os.path.join(RESULTS_DIR, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "                results_file = os.path.join(model_results_dir, \"best_model_results.csv\")\n",
    "\n",
    "                # Skip if results file doesn't exist\n",
    "                if not os.path.exists(results_file):\n",
    "                    print(f\"Results file for {model_name} not found. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Load results from CSV\n",
    "                results = pd.read_csv(results_file)\n",
    "                maes = results[\"test_mae\"].values\n",
    "                rmses = results[\"test_rmse\"].values\n",
    "\n",
    "                # Add results to the dataframe\n",
    "                for i in range(len(maes)):  # Assuming results have folds\n",
    "                    results_df = pd.concat([\n",
    "                        results_df,\n",
    "                        pd.DataFrame([{\n",
    "                            \"model\": baseline_type,\n",
    "                            \"input\": input_name,\n",
    "                            \"fold\": i + 1,\n",
    "                            \"mae\": maes[i],\n",
    "                            \"rmse\": rmses[i]\n",
    "                        }])\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            n_trials = 100\n",
    "            num_epochs = 500\n",
    "\n",
    "            model_name = f\"{model_type}{input_name}\"\n",
    "            print(f\"Processing Model: {model_name}\")\n",
    "\n",
    "            # Construct the results directory path\n",
    "            model_results_dir = os.path.join(RESULTS_DIR, f\"{model_name}_{n_trials}_trials_{num_epochs}_epochs\")\n",
    "            results_file = os.path.join(model_results_dir, \"best_model_results.csv\")\n",
    "\n",
    "            # Skip if results file doesn't exist\n",
    "            if not os.path.exists(results_file):\n",
    "                print(f\"Results file for {model_name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Load results from CSV\n",
    "            results = pd.read_csv(results_file)\n",
    "            maes = results[\"test_mae\"].values\n",
    "            rmses = results[\"test_rmse\"].values\n",
    "\n",
    "            # Add results to the dataframe\n",
    "            for i in range(len(maes)):  # Assuming results have folds\n",
    "                results_df = pd.concat([\n",
    "                    results_df,\n",
    "                    pd.DataFrame([{\n",
    "                        \"model\": model_type,\n",
    "                        \"input\": input_name,\n",
    "                        \"fold\": i + 1,\n",
    "                        \"mae\": maes[i],\n",
    "                        \"rmse\": rmses[i]\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model           input fold       mae       rmse\n",
      "0          BaselineAverage         Indexes  3.0  7.061335   9.466859\n",
      "1   BaselineLastDifference         Indexes  3.0  4.363204   7.537955\n",
      "2                      CNN         Indexes  3.0  0.345493   0.555321\n",
      "3                      CNN          Series  3.0  6.417948   8.568752\n",
      "4                      CNN  Series_Masking  3.0  0.701390   0.967899\n",
      "5                     FFNN         Indexes  3.0  0.549793   1.042796\n",
      "6                     FFNN          Series  3.0  8.937057  11.175140\n",
      "7                     FFNN  Series_Masking  3.0  1.012417   1.686616\n",
      "8                     LSTM         Indexes  3.0  0.148808   0.186667\n",
      "9                     LSTM          Series  3.0  4.428348   7.396018\n",
      "10                    LSTM  Series_Masking  3.0  1.099881   2.904646\n",
      "11             Transformer         Indexes  3.0  0.403696   0.533269\n",
      "12             Transformer          Series  3.0  1.750024   3.790114\n",
      "13             Transformer  Series_Masking  3.0  0.553271   0.790670\n"
     ]
    }
   ],
   "source": [
    "#average fold results for each model and input\n",
    "avg_results_df = results_df.groupby([\"model\", \"input\"]).mean().reset_index()\n",
    "print(avg_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_61335/1915286810.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rmse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Filter data for the selected input type including baseline models even if they don't have the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mdf_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput_type\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Get the unique models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch_motifpredenv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"\u001b[0m\u001b[0;34mThe truth value of a \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m is ambiguous. \u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "for input_type in inputs:\n",
    "    for metric in [\"mae\", \"rmse\"]:\n",
    "        # Filter data for the selected input type and always include baseline models\n",
    "        df_filtered = results_df[(results_df['input'] == input_type) | (results_df['model'].str.contains(\"Baseline\"))]\n",
    "\n",
    "        # Get the unique models\n",
    "        models = df_filtered['model'].unique()\n",
    "\n",
    "        # Initialize an empty matrix for p-values\n",
    "        pval_matrix = pd.DataFrame(np.nan, index=models, columns=models)\n",
    "\n",
    "        # Fill the matrix with p-values\n",
    "        for i, model1 in enumerate(models):\n",
    "            for j, model2 in enumerate(models):\n",
    "                if i < j:  # Avoid redundant comparisons (matrix is symmetric)\n",
    "                    data1 = df_filtered[df_filtered['model'] == model1].sort_values('fold')[metric]\n",
    "                    data2 = df_filtered[df_filtered['model'] == model2].sort_values('fold')[metric]\n",
    "                    \n",
    "                    # Perform a paired t-test\n",
    "                    if len(data1) == len(data2):  # Ensure the lengths match for paired t-test\n",
    "                        t_stat, p_value = ttest_rel(data1, data2, alternative='less')\n",
    "                        \n",
    "                        # Populate the matrix\n",
    "                        pval_matrix.loc[model1, model2] = p_value\n",
    "                        pval_matrix.loc[model2, model1] = 1 - p_value\n",
    "\n",
    "        print(\"P-Value Matrix:\", input_type, metric)\n",
    "        print(pval_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_motifpredenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
